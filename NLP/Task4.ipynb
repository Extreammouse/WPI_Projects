{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai google-generativeai langchain langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lGWRt4cS-ea_",
        "outputId": "3970bbd1-8d5b-4edc-eb12-93e4aebda9a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.55)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.17)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.3)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "Installing collected packages: google-ai-generativelanguage, google-generativeai, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.17\n",
            "    Uninstalling google-ai-generativelanguage-0.6.17:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.17\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.8.4\n",
            "    Uninstalling google-generativeai-0.8.4:\n",
            "      Successfully uninstalled google-generativeai-0.8.4\n",
            "  Attempting uninstall: langchain-google-genai\n",
            "    Found existing installation: langchain-google-genai 2.1.3\n",
            "    Uninstalling langchain-google-genai-2.1.3:\n",
            "      Successfully uninstalled langchain-google-genai-2.1.3\n",
            "Successfully installed google-ai-generativelanguage-0.6.15 google-generativeai-0.8.5 langchain-google-genai-2.0.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "4a5dd3c155bb4dc2af5998e65464cfc9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "tdOvY620-XcX",
        "outputId": "5554a218-2f4d-4c00-c5eb-869f5d3a0993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💡 Using model → gemini-1.5-flash\n",
            "Model test: content='Hi there! How can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-8056f736-a8e7-4c8e-9473-4f1bf8b689c7-0' usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}}\n",
            "Testing Sequential Chain 1...\n",
            "\n",
            "--- CHAIN 1 RESULTS ---\n",
            "\n",
            "PROBLEM:\n",
            "Core problem: Low ML model accuracy\n",
            "\n",
            "CONTEXT:\n",
            "To diagnose and solve low ML model accuracy, we need background information across several areas:\n",
            "\n",
            "**1. Data:**\n",
            "\n",
            "* **Data Quality:**\n",
            "    * **Completeness:** Are there missing values? How are they handled (imputation, removal)?  What's the percentage of missing data?  Are there systematic biases in missingness?\n",
            "    * **Accuracy:** Is the data correct? Are there errors, outliers, or inconsistencies?  What's the process for data validation and cleaning?\n",
            "    * **Relevance:** Is the data relevant to the problem being solved? Are there irrelevant or redundant features?\n",
            "    * **Representativeness:** Does the data accurately represent the real-world scenario the model will be deployed in? Is there a sampling bias?  Does it cover the full range of expected inputs?\n",
            "    * **Consistency:** Is the data formatted consistently (e.g., units, data types)?\n",
            "* **Data Size:** How much data is available? Is it sufficient for the complexity of the model and the problem?  Is there a class imbalance?\n",
            "* **Data Distribution:** What is the distribution of the features and target variable? Are there skewed distributions that might affect model performance?  Are there any unusual patterns or clusters?\n",
            "* **Data Preprocessing:** What preprocessing steps were taken (e.g., scaling, normalization, encoding categorical variables)?  Were appropriate techniques used for the data type and model?\n",
            "* **Feature Engineering:** What features were used? Were relevant features missed? Were irrelevant features included?  Were features engineered effectively to capture underlying patterns?\n",
            "\n",
            "\n",
            "**2. Model:**\n",
            "\n",
            "* **Model Choice:** What type of model was used (e.g., linear regression, decision tree, neural network)? Was it the appropriate choice for the data and problem?\n",
            "* **Model Complexity:** Is the model too simple (underfitting) or too complex (overfitting)?  How many parameters does it have?\n",
            "* **Hyperparameters:** What hyperparameters were used? Were they tuned effectively using techniques like cross-validation or grid search?\n",
            "* **Training Process:** How was the model trained? What was the learning rate, batch size, number of epochs?  Were appropriate optimization algorithms used?  Were early stopping techniques employed?\n",
            "* **Evaluation Metrics:** What metrics were used to evaluate the model (e.g., accuracy, precision, recall, F1-score, AUC)?  Are these metrics appropriate for the problem?  Were different evaluation metrics considered?\n",
            "\n",
            "\n",
            "**3. Deployment Environment:**\n",
            "\n",
            "* **Data Drift:** Does the distribution of data in the deployment environment differ significantly from the training data?\n",
            "* **System Constraints:** Are there any limitations in the deployment environment (e.g., memory, processing power) that might affect model performance?\n",
            "\n",
            "\n",
            "**4. Debugging Strategies:**\n",
            "\n",
            "* **Error Analysis:**  Analyze the model's predictions on incorrectly classified instances.  What are the common characteristics of these errors?\n",
            "* **Visualization:** Use visualizations to understand the data, model behavior, and feature importance.\n",
            "* **Feature Importance:** Determine which features are most important for the model's predictions.  Are there any unexpected or counterintuitive results?\n",
            "* **Cross-Validation:**  Use cross-validation to get a more robust estimate of model performance and identify potential overfitting or underfitting.\n",
            "\n",
            "\n",
            "By systematically investigating these areas, you can pinpoint the root cause of the low accuracy and implement appropriate solutions.  Remember to document your findings and the steps you take to address the problem.\n",
            "\n",
            "SOLUTION:\n",
            "## Comprehensive Solution for Low ML Model Accuracy\n",
            "\n",
            "Addressing low ML model accuracy requires a systematic investigation across data, model, and deployment aspects.  The following outlines a structured approach, mirroring the provided context:\n",
            "\n",
            "**Phase 1: Data Analysis & Preprocessing**\n",
            "\n",
            "1. **Data Quality Assessment:**\n",
            "    * **Completeness:** Analyze missing values using heatmaps and summary statistics. Identify patterns of missingness (e.g., missing completely at random (MCAR), missing at random (MAR), missing not at random (MNAR)).  Handle missing data appropriately: imputation (e.g., mean, median, KNN, MICE) for MCAR/MAR, or specialized techniques for MNAR, potentially involving feature engineering to capture the missingness pattern. Document the percentage of missing data before and after handling.\n",
            "    * **Accuracy:** Perform data validation checks (e.g., range checks, consistency checks, plausibility checks). Identify and correct or remove outliers using techniques like Z-score or IQR methods. Document the data cleaning process and the number of outliers removed.\n",
            "    * **Relevance & Redundancy:** Calculate feature correlations (Pearson, Spearman) to identify redundant features. Use feature selection techniques (e.g., filter methods like correlation, wrapper methods like recursive feature elimination, embedded methods like LASSO/Ridge regression) to select the most relevant features.\n",
            "    * **Representativeness & Sampling Bias:** Analyze the data distribution to check for representativeness. If sampling bias is suspected, consider techniques like stratified sampling or weighting to correct for it.  Visualize the data distribution using histograms, box plots, and scatter plots to identify potential biases.\n",
            "    * **Consistency:** Ensure consistent data types and units.  Standardize or normalize data as needed.\n",
            "\n",
            "2. **Data Size & Distribution:**\n",
            "    * **Size:** Determine if the dataset size is sufficient for the model complexity.  If not, consider data augmentation techniques or exploring simpler models.\n",
            "    * **Class Imbalance:** If dealing with classification, check for class imbalance. Address this using techniques like oversampling (SMOTE), undersampling, or cost-sensitive learning.\n",
            "    * **Distribution:** Visualize feature and target variable distributions.  Transform skewed distributions using techniques like log transformation, Box-Cox transformation, or Yeo-Johnson transformation to improve model performance.\n",
            "\n",
            "3. **Data Preprocessing & Feature Engineering:**\n",
            "    * **Preprocessing:** Apply appropriate scaling (e.g., standardization, min-max scaling) and encoding (e.g., one-hot encoding, label encoding) based on the model and data type.\n",
            "    * **Feature Engineering:** Create new features from existing ones to potentially improve model performance.  This might involve combining features, creating interaction terms, or extracting features from text or images.  Document all feature engineering steps.\n",
            "\n",
            "\n",
            "**Phase 2: Model Selection, Training, & Evaluation**\n",
            "\n",
            "1. **Model Choice:** Select an appropriate model based on the problem type (classification, regression), data characteristics, and interpretability requirements.  Consider different model types (linear models, tree-based models, neural networks, support vector machines) and compare their performance.\n",
            "\n",
            "2. **Model Complexity & Hyperparameter Tuning:**\n",
            "    * **Complexity:** Start with a simpler model and gradually increase complexity. Monitor performance to avoid overfitting or underfitting.  Use techniques like regularization (L1, L2) to prevent overfitting.\n",
            "    * **Hyperparameter Tuning:** Use techniques like grid search, random search, or Bayesian optimization to find optimal hyperparameters.  Employ cross-validation (k-fold, stratified k-fold) to obtain a robust estimate of model performance and avoid overfitting to the training data.\n",
            "\n",
            "3. **Training Process:**\n",
            "    * **Optimization:** Choose an appropriate optimization algorithm (e.g., gradient descent, Adam, RMSprop).  Monitor the training and validation loss curves to detect overfitting or underfitting.\n",
            "    * **Learning Rate, Batch Size, Epochs:** Experiment with different learning rates, batch sizes, and numbers of epochs to find the optimal settings.  Use early stopping to prevent overfitting.\n",
            "\n",
            "4. **Evaluation Metrics:**\n",
            "    * **Selection:** Choose appropriate evaluation metrics based on the problem type and business goals (e.g., accuracy, precision, recall, F1-score, AUC, RMSE, MAE).  Consider using a combination of metrics to get a comprehensive understanding of model performance.\n",
            "    * **Comparison:** Compare the performance of different models and hyperparameter settings using the chosen evaluation metrics.\n",
            "\n",
            "\n",
            "**Phase 3: Deployment & Monitoring**\n",
            "\n",
            "1. **Data Drift:** Monitor the data distribution in the deployment environment.  If significant drift is detected, retrain the model with updated data or implement techniques to adapt to the changing data distribution (e.g., concept drift detection and adaptation).\n",
            "\n",
            "2. **System Constraints:** Ensure the model meets the deployment environment's constraints (memory, processing power).  Consider model compression techniques or using less computationally intensive models if necessary.\n",
            "\n",
            "\n",
            "**Phase 4: Debugging & Refinement**\n",
            "\n",
            "1. **Error Analysis:** Analyze misclassified instances to identify patterns and potential data issues or model limitations.  Create confusion matrices and analyze the types of errors made.\n",
            "\n",
            "2. **Visualization:** Use visualizations (e.g., ROC curves, precision-recall curves, feature importance plots) to understand model behavior and identify areas for improvement.\n",
            "\n",
            "3. **Feature Importance:** Analyze feature importance to understand which features are most influential.  This can help identify missing features or irrelevant features.\n",
            "\n",
            "4. **Iterative Refinement:** Based on the findings from the debugging steps, iterate on the data preprocessing, feature engineering, model selection, and hyperparameter tuning to improve model accuracy.  Document all changes and their impact on model performance.\n",
            "\n",
            "\n",
            "This comprehensive approach, combining rigorous data analysis, careful model selection and training, and thorough evaluation and debugging, will significantly increase the chances of achieving high accuracy in your ML model. Remember to meticulously document each step, facilitating reproducibility and future improvements.\n",
            "\n",
            "Testing Sequential Chain 2...\n",
            "\n",
            "--- CHAIN 2 RESULTS ---\n",
            "\n",
            "SUMMARY:\n",
            "The user needs guidance on deploying a Flask application to AWS using CI/CD.\n",
            "\n",
            "CLARIFYING_QUESTIONS:\n",
            "1. What is the current state of your Flask application?  (e.g., Is it already containerized?  Do you have a version control system like Git in place?  What are the application's dependencies?)\n",
            "\n",
            "2. What is your desired CI/CD pipeline architecture? (e.g., Are you aiming for a simple setup using AWS services like CodePipeline and CodeBuild, or do you prefer a more complex solution involving other tools like Jenkins or GitLab CI?)\n",
            "\n",
            "3. What are your deployment requirements and preferences? (e.g.,  Will the application be deployed to EC2, Elastic Beanstalk, ECS, EKS, or a different AWS service? Do you have specific scaling or security requirements?)\n",
            "\n",
            "ACTION_PLAN:\n",
            "To create a comprehensive action plan, we need answers to these clarifying questions:\n",
            "\n",
            "1. **What is the current state of your Flask application?** (e.g.,  Is it already containerized (Docker)?  Does it have a requirements.txt file?  Is it version controlled (Git)? What is the application's size and expected traffic?)\n",
            "\n",
            "2. **What CI/CD tools are you comfortable using or prefer?** (e.g., GitHub Actions, GitLab CI, AWS CodePipeline, Jenkins)  This will heavily influence the specifics of the steps.\n",
            "\n",
            "3. **What AWS services do you want to use for deployment?** (e.g., EC2, Elastic Beanstalk, ECS, EKS, Lambda, Fargate).  Each has different deployment strategies and complexities.\n",
            "\n",
            "\n",
            "**Draft Action Plan (Assuming Docker, GitHub Actions, and Elastic Beanstalk):**\n",
            "\n",
            "This plan assumes a basic level of familiarity with Docker, GitHub, and AWS.  Adjust based on your answers to the clarifying questions above.\n",
            "\n",
            "**Phase 1: Setup and Configuration**\n",
            "\n",
            "1. **Version Control:** Ensure your Flask application is in a Git repository (e.g., GitHub).  Commit all code, including a `requirements.txt` file listing project dependencies.\n",
            "\n",
            "2. **Dockerize your application:**\n",
            "    * Create a `Dockerfile` to build a Docker image of your application. This should include instructions to install dependencies, copy your application code, and define the entry point for your Flask app.  Example:\n",
            "\n",
            "    ```dockerfile\n",
            "    FROM python:3.9-slim-buster\n",
            "\n",
            "    WORKDIR /app\n",
            "\n",
            "    COPY requirements.txt requirements.txt\n",
            "    RUN pip install -r requirements.txt\n",
            "\n",
            "    COPY . .\n",
            "\n",
            "    CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"your_app:app\"]\n",
            "    ```\n",
            "    * Build the Docker image locally: `docker build -t my-flask-app .`\n",
            "    * Test the Docker image locally: `docker run -p 5000:5000 my-flask-app`\n",
            "\n",
            "3. **AWS Setup:**\n",
            "    * Create an AWS account (if you don't already have one).\n",
            "    * Create an IAM user with appropriate permissions to interact with Elastic Beanstalk and ECR (Elastic Container Registry).  Use the principle of least privilege.\n",
            "    * Create an Elastic Beanstalk application.\n",
            "\n",
            "4. **ECR Setup:**\n",
            "    * Create an ECR repository to store your Docker images.\n",
            "\n",
            "**Phase 2: CI/CD Pipeline with GitHub Actions**\n",
            "\n",
            "1. **GitHub Actions Workflow:** Create a `.github/workflows/deploy.yml` file in your repository. This file will define the CI/CD pipeline.  Example:\n",
            "\n",
            "```yaml\n",
            "name: Deploy to AWS Elastic Beanstalk\n",
            "\n",
            "on:\n",
            "  push:\n",
            "    branches:\n",
            "      - main  # Or your main branch\n",
            "\n",
            "jobs:\n",
            "  build:\n",
            "    runs-on: ubuntu-latest\n",
            "    steps:\n",
            "      - name: Checkout code\n",
            "        uses: actions/checkout@v3\n",
            "\n",
            "      - name: Build Docker image\n",
            "        run: docker build -t my-flask-app .\n",
            "\n",
            "      - name: Login to ECR\n",
            "        uses: amazon/aws-cli@v1\n",
            "        with:\n",
            "          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
            "          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
            "          aws-region: ${{ secrets.AWS_REGION }}\n",
            "\n",
            "      - name: Tag and push Docker image\n",
            "        run: |\n",
            "          docker tag my-flask-app <your-ecr-repo-url>:latest\n",
            "          docker push <your-ecr-repo-url>:latest\n",
            "\n",
            "  deploy:\n",
            "    runs-on: ubuntu-latest\n",
            "    needs: build\n",
            "    steps:\n",
            "      - name: Deploy to Elastic Beanstalk\n",
            "        uses: einarsson/aws-eb-deploy@v2\n",
            "        with:\n",
            "          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
            "          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
            "          application_name: <your-eb-app-name>\n",
            "          environment_name: <your-eb-env-name>\n",
            "          region: ${{ secrets.AWS_REGION }}\n",
            "          version_label: latest\n",
            "          docker_image: <your-ecr-repo-url>:latest\n",
            "\n",
            "```\n",
            "\n",
            "2. **AWS Credentials in GitHub Secrets:** Add your AWS access key ID and secret access key as secrets in your GitHub repository settings.  **Important:**  Never hardcode these credentials directly in your code.\n",
            "\n",
            "**Phase 3: Testing and Deployment**\n",
            "\n",
            "1. **Commit and Push:** Commit your changes (Dockerfile, GitHub Actions workflow) and push them to your GitHub repository.\n",
            "\n",
            "2. **Monitor Deployment:** GitHub Actions will automatically trigger the build and deployment process. Monitor the logs in GitHub Actions to ensure the deployment is successful.\n",
            "\n",
            "3. **Testing:** Thoroughly test your deployed application.\n",
            "\n",
            "\n",
            "This is a high-level plan.  You'll need to adapt it based on your specific needs and chosen AWS services.  Remember to consult the official documentation for Docker, GitHub Actions, and AWS Elastic Beanstalk for detailed instructions and best practices.  Consider using a more robust CI/CD solution like AWS CodePipeline for larger, more complex applications.\n",
            "\n",
            "Testing Custom Chain 3...\n",
            "\n",
            "--- CUSTOM CHAIN 3 RESULTS ---\n",
            "\n",
            "ANALYSIS:\n",
            " SQL databases prioritize data integrity and consistency through ACID properties and structured schemas, offering robust transaction management and data validation.  However, they can be less flexible for handling evolving data structures and may struggle with high-volume, unstructured data. NoSQL databases prioritize scalability and flexibility, accommodating diverse data models and high write throughput.  They often sacrifice ACID properties for performance, potentially leading to data inconsistency in some scenarios. The choice depends on the specific application needs, balancing data integrity requirements with scalability and flexibility demands.\n",
            "\n",
            "ANSWER:\n",
            " The key trade-off is between data consistency and scalability. SQL databases offer strong consistency but can struggle with scale, while NoSQL databases prioritize scalability but may compromise consistency.\n"
          ]
        }
      ],
      "source": [
        "# Task 4: LangChain Practice with Gemini\n",
        "# --------------------------------------\n",
        "# Requires:  pip install -U langchain-google-genai google-generativeai\n",
        "\n",
        "import os, logging\n",
        "from typing import Dict, Any\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "\n",
        "class QA(BaseModel):\n",
        "    analysis: str\n",
        "    answer: str\n",
        "\n",
        "json_parser = PydanticOutputParser(pydantic_object=QA)\n",
        "single_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Return ONLY valid JSON that matches this schema:\\n{schema}\"),\n",
        "    (\"human\", \"QUESTION: {question}\")\n",
        "]).partial(schema=json_parser.get_format_instructions())\n",
        "\n",
        "llm_flash = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
        "\n",
        "custom_chain = single_prompt | llm_flash | json_parser\n",
        "# ----------------------------------------------------------------------\n",
        "# 0.  Logging\n",
        "# ----------------------------------------------------------------------\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1.  API key (kept exactly as you wrote it)\n",
        "# ----------------------------------------------------------------------\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDjIU4SWDbq9OXkRjkK2j7EV7t5lBHwdrU\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2.  Pick an active Gemini model\n",
        "# ----------------------------------------------------------------------\n",
        "def list_available_models() -> list[str]:\n",
        "    try:\n",
        "        return [\n",
        "            m.name.replace(\"models/\", \"\")      # strip prefix if present\n",
        "            for m in genai.list_models()\n",
        "            if \"gemini\" in m.name.lower() and \"1.0\" not in m.name.lower()\n",
        "        ]\n",
        "    except Exception as exc:\n",
        "        logging.error(\"Could not list models: %s\", exc)\n",
        "        return []\n",
        "\n",
        "available = list_available_models()\n",
        "model_name = next((m for m in available if m == \"gemini-1.5-flash\"),\n",
        "                  available[0] if available else \"gemini-1.5-flash\")\n",
        "print(f\"💡 Using model → {model_name}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3.  Initialise LLM\n",
        "# ----------------------------------------------------------------------\n",
        "llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.4)\n",
        "print(\"Model test:\", llm.invoke(\"Hi!\"))\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# ======================================================================\n",
        "# SEQUENTIAL CHAIN 1  ---------------------------------------------------\n",
        "# ======================================================================\n",
        "problem_prompt  = ChatPromptTemplate.from_template(\n",
        "    \"Given the following user query, extract the core problem:\\n{query}\\n\\nCore problem:\")\n",
        "context_prompt  = ChatPromptTemplate.from_template(\n",
        "    \"Problem:\\n{problem}\\n\\nGenerate background information helpful to solve it:\")\n",
        "solution_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}\\nContext: {context}\\n\\nProvide a comprehensive solution:\")\n",
        "\n",
        "problem_chain   = problem_prompt  | llm | output_parser\n",
        "context_chain   = context_prompt  | llm | output_parser\n",
        "solution_chain  = solution_prompt | llm | output_parser\n",
        "\n",
        "def sequential_chain_1(inputs: Dict[str, Any]) -> Dict[str, str]:\n",
        "    try:\n",
        "        problem  = problem_chain.invoke({\"query\": inputs[\"query\"]})\n",
        "        context  = context_chain.invoke({\"problem\": problem})\n",
        "        solution = solution_chain.invoke({\"problem\": problem, \"context\": context})\n",
        "        return {\"problem\": problem, \"context\": context, \"solution\": solution}\n",
        "    except Exception as exc:\n",
        "        logging.error(\"Chain 1 error: %s\", exc)\n",
        "        return {\"problem\": f\"Error: {exc}\", \"context\": \"\", \"solution\": \"\"}\n",
        "\n",
        "def test_chain_1():\n",
        "    res = sequential_chain_1({\"query\": \"How can I improve my ML model's accuracy?\"})\n",
        "    print(\"\\n--- CHAIN 1 RESULTS ---\")\n",
        "    for k, v in res.items():\n",
        "        print(f\"\\n{k.upper()}:\\n{v}\")\n",
        "\n",
        "# ======================================================================\n",
        "# SEQUENTIAL CHAIN 2  ---------------------------------------------------\n",
        "#   Summary  ->  Clarifying questions  ->  Action plan\n",
        "# ======================================================================\n",
        "summary_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Summarise the user's request in one sentence:\\n{query}\\n\\nSummary:\")\n",
        "clarify_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Summary: {summary}\\n\\nAsk three clarifying questions you would need answered:\")\n",
        "plan_prompt    = ChatPromptTemplate.from_template(\n",
        "    \"Summary: {summary}\\nAnswers to clarifying questions: {answers}\\n\\n\"\n",
        "    \"Draft a step‑by‑step action plan:\")\n",
        "\n",
        "summary_chain  = summary_prompt  | llm | output_parser\n",
        "clarify_chain  = clarify_prompt  | llm | output_parser\n",
        "plan_chain     = plan_prompt    | llm | output_parser\n",
        "\n",
        "def sequential_chain_2(inputs: Dict[str, Any]) -> Dict[str, str]:\n",
        "    try:\n",
        "        summary  = summary_chain.invoke({\"query\": inputs[\"query\"]})\n",
        "        questions = clarify_chain.invoke({\"summary\": summary})\n",
        "        # In a real system you'd collect user answers; here we'll stub them.\n",
        "        stub_answers = \"1. ___\\n2. ___\\n3. ___\"\n",
        "        plan = plan_chain.invoke({\"summary\": summary, \"answers\": stub_answers})\n",
        "        return {\"summary\": summary, \"clarifying_questions\": questions, \"action_plan\": plan}\n",
        "    except Exception as exc:\n",
        "        logging.error(\"Chain 2 error: %s\", exc)\n",
        "        return {\"summary\": \"\", \"clarifying_questions\": \"\", \"action_plan\": f\"Error: {exc}\"}\n",
        "\n",
        "def test_chain_2():\n",
        "    res = sequential_chain_2({\"query\": \"I need to deploy my Flask app on AWS with CI/CD; what should I do?\"})\n",
        "    print(\"\\n--- CHAIN 2 RESULTS ---\")\n",
        "    for k, v in res.items():\n",
        "        print(f\"\\n{k.upper()}:\\n{v}\")\n",
        "\n",
        "# =====================================================================\n",
        "# CUSTOM CHAIN 3  – single call, strict JSON, auto‑parsed\n",
        "# =====================================================================\n",
        "\n",
        "# 1) pydantic schema for the expected JSON\n",
        "# First, define your QA class\n",
        "class QA(BaseModel):\n",
        "    analysis: str = Field(description=\"Key challenges extracted from the question\")\n",
        "    answer: str = Field(description=\"Concise recommendation/answer\")\n",
        "\n",
        "# Initialize the parser\n",
        "json_parser = PydanticOutputParser(pydantic_object=QA)\n",
        "\n",
        "# Initialize llm_flash BEFORE using it in custom_chain\n",
        "llm_flash = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.4)\n",
        "\n",
        "# Now create the prompt template and chain\n",
        "single_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Return ONLY valid JSON that matches this schema:\\n{schema}\"),\n",
        "    (\"human\", \"QUESTION: {question}\")\n",
        "]).partial(schema=json_parser.get_format_instructions())\n",
        "\n",
        "# Now this line will work because llm_flash is defined\n",
        "custom_chain = single_prompt | llm_flash | json_parser\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# test helper\n",
        "# ---------------------------------------------------------------------\n",
        "def test_custom_chain():\n",
        "    try:\n",
        "        res: QA = custom_chain.invoke({\"question\": \"What are the trade‑offs between SQL and NoSQL databases?\"})\n",
        "        print(\"\\n--- CUSTOM CHAIN 3 RESULTS ---\")\n",
        "        print(\"\\nANALYSIS:\\n\", res.analysis)\n",
        "        print(\"\\nANSWER:\\n\",   res.answer)\n",
        "    except Exception as exc:\n",
        "        print(f\"Chain 3 failed: {exc}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# MAIN\n",
        "# ----------------------------------------------------------------------\n",
        "def main():\n",
        "    print(\"Testing Sequential Chain 1...\")\n",
        "    test_chain_1()\n",
        "\n",
        "    print(\"\\nTesting Sequential Chain 2...\")\n",
        "    test_chain_2()\n",
        "\n",
        "    print(\"\\nTesting Custom Chain 3...\")\n",
        "    test_custom_chain()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}