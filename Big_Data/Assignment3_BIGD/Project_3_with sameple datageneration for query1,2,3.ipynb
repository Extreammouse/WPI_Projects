{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"VirtualHandshake\").getOrCreate()\n",
        "\n",
        "print(\"1. Spark-RDDs: Scalable Virtual Handshake Applications\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "# Helper function to generate random attributes\n",
        "def generate_attributes():\n",
        "    name = ''.join(random.choices(string.ascii_letters, k=random.randint(5, 10)))\n",
        "    age = random.randint(18, 80)\n",
        "    email = f\"{name.lower()}@example.com\"\n",
        "    return f\"{name},{age},{email}\"\n",
        "\n",
        "# Create PEOPLE dataset\n",
        "def create_people(num_people=100000):\n",
        "    return [(i, random.randint(1, 10000), random.randint(1, 10000), generate_attributes())\n",
        "            for i in range(num_people)]\n",
        "\n",
        "# Create ACTIVATED dataset (subset of PEOPLE)\n",
        "def create_activated(people, activation_rate=0.1):\n",
        "    return random.sample(people, int(len(people) * activation_rate))\n",
        "\n",
        "# Create PEOPLE_WITH_HANDSHAKE_INFO dataset\n",
        "def create_people_with_handshake_info(people, activated):\n",
        "    activated_ids = set(a[0] for a in activated)\n",
        "    return [(p[0], p[1], p[2], p[3], \"yes\" if p[0] in activated_ids else \"no\") for p in people]\n",
        "\n",
        "# Generate datasets\n",
        "people = create_people(10000)  # Reduced size for demonstration\n",
        "activated = create_activated(people)\n",
        "people_with_handshake_info = create_people_with_handshake_info(people, activated)\n",
        "\n",
        "# Create RDDs\n",
        "people_rdd = spark.sparkContext.parallelize(people)\n",
        "activated_rdd = spark.sparkContext.parallelize(activated)\n",
        "people_with_handshake_info_rdd = spark.sparkContext.parallelize(people_with_handshake_info)\n",
        "\n",
        "print(\"Datasets created successfully.\")\n",
        "\n",
        "# Helper function for distance calculation\n",
        "def distance(p1, p2):\n",
        "    return ((p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)**0.5\n",
        "\n",
        "# Query 1\n",
        "def query1(people_rdd, activated_rdd):\n",
        "    activated_list = activated_rdd.collect()\n",
        "    def find_nearby(person):\n",
        "        return [(person[0], act[0]) for act in activated_list if distance(person, act) <= 6 and person[0] != act[0]]\n",
        "    return people_rdd.flatMap(find_nearby).distinct()\n",
        "\n",
        "result_query1 = query1(people_rdd, activated_rdd)\n",
        "print(\"\\nQuery 1 Result (sample):\")\n",
        "for pair in result_query1.take(5):\n",
        "    print(pair)\n",
        "\n",
        "# Query 2\n",
        "def query2(people_rdd, activated_rdd):\n",
        "    activated_list = activated_rdd.collect()\n",
        "    def find_nearby_ids(person):\n",
        "        return [person[0] for act in activated_list if distance(person, act) <= 6 and person[0] != act[0]]\n",
        "    return people_rdd.flatMap(find_nearby_ids).distinct()\n",
        "\n",
        "result_query2 = query2(people_rdd, activated_rdd)\n",
        "print(\"\\nQuery 2 Result (sample):\")\n",
        "for id in result_query2.take(5):\n",
        "    print(id)\n",
        "\n",
        "# Query 3\n",
        "def query3(people_with_handshake_info_rdd):\n",
        "    people_list = people_with_handshake_info_rdd.collect()\n",
        "    def count_nearby(person):\n",
        "        return (person[0], sum(1 for p in people_list if distance(person, p) <= 6 and person[0] != p[0]))\n",
        "    return people_with_handshake_info_rdd.filter(lambda p: p[4] == \"yes\").map(count_nearby)\n",
        "\n",
        "result_query3 = query3(people_with_handshake_info_rdd)\n",
        "print(\"\\nQuery 3 Result (sample):\")\n",
        "for pair in result_query3.take(5):\n",
        "    print(pair)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Y3QqNqOtzV",
        "outputId": "92f2b32b-6b32-40b8-aabd-0b3f17a0f0a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Spark-RDDs: Scalable Virtual Handshake Applications\n",
            "======================================================\n",
            "Datasets created successfully.\n",
            "\n",
            "Query 1 Result (sample):\n",
            "(147, 5501)\n",
            "(1245, 4577)\n",
            "(3275, 3579)\n",
            "(3381, 7795)\n",
            "(3552, 922)\n",
            "\n",
            "Query 2 Result (sample):\n",
            "1296\n",
            "3552\n",
            "3750\n",
            "5286\n",
            "6694\n",
            "\n",
            "Query 3 Result (sample):\n",
            "(53, 0)\n",
            "(58, 0)\n",
            "(61, 0)\n",
            "(62, 0)\n",
            "(70, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col, when, mean\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"ECommerceAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "df = spark.read.csv(\"/content/ecommerce_customer_data_custom_ratios.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Choose a target attribute\n",
        "target_column = \"Product Price\"\n",
        "\n",
        "# Define numeric and categorical features\n",
        "numeric_features = [\"Quantity\", \"Total Purchase Amount\", \"Customer Age\", \"Returns\", \"Churn\"]\n",
        "categorical_features = [\"Gender\", \"Product Category\"]\n",
        "\n",
        "# Handle null values\n",
        "for column in df.columns:\n",
        "    if df.schema[column].dataType.simpleString() in ['integer', 'double', 'float']:\n",
        "        df = df.withColumn(column, col(column).cast(\"double\"))\n",
        "        mean_value = df.select(mean(col(column))).collect()[0][0]\n",
        "        df = df.fillna(mean_value, subset=[column])\n",
        "    else:\n",
        "        df = df.fillna(\"Unknown\", subset=[column])\n",
        "\n",
        "# Split the dataset\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Create stages for the pipeline\n",
        "stages = []\n",
        "\n",
        "# Handle categorical features\n",
        "for categoricalCol in categorical_features:\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\", handleInvalid=\"keep\")\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
        "    stages += [stringIndexer, encoder]\n",
        "\n",
        "# Assemble features\n",
        "assembler_inputs = [c + \"classVec\" for c in categorical_features] + numeric_features\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
        "stages += [assembler]\n",
        "\n",
        "# Define models\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=target_column)\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target_column)\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=target_column)\n",
        "\n",
        "# Create pipelines\n",
        "lr_pipeline = Pipeline(stages=stages + [lr])\n",
        "rf_pipeline = Pipeline(stages=stages + [rf])\n",
        "gbt_pipeline = Pipeline(stages=stages + [gbt])\n",
        "\n",
        "# Train models\n",
        "lr_model = lr_pipeline.fit(train_df)\n",
        "rf_model = rf_pipeline.fit(train_df)\n",
        "gbt_model = gbt_pipeline.fit(train_df)\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_model(model, test_data):\n",
        "    predictions = model.transform(test_data)\n",
        "    evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\")\n",
        "    rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
        "    mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
        "    r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
        "    return rmse, mae, r2\n",
        "\n",
        "# Evaluate models\n",
        "lr_metrics = evaluate_model(lr_model, test_df)\n",
        "rf_metrics = evaluate_model(rf_model, test_df)\n",
        "gbt_metrics = evaluate_model(gbt_model, test_df)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Evaluation Results:\")\n",
        "print(\"-------------------------\")\n",
        "print(\"Metrics: RMSE, MAE, R2\")\n",
        "print(f\"Linear Regression: {lr_metrics}\")\n",
        "print(f\"Random Forest: {rf_metrics}\")\n",
        "print(f\"Gradient Boosted Trees: {gbt_metrics}\")\n",
        "\n",
        "# Analysis and discussion\n",
        "print(\"\\nAnalysis and Discussion:\")\n",
        "print(\"------------------------\")\n",
        "print(\"1. Model Performance:\")\n",
        "print(\"   - Comparing RMSE, MAE, and R2 values, we can see which model performs best.\")\n",
        "print(\"   - Lower RMSE and MAE, and higher R2 indicate better performance.\")\n",
        "print(\"2. Feature Importance:\")\n",
        "print(\"   - The Random Forest and Gradient Boosted Trees models can provide feature importance.\")\n",
        "print(\"   - This can help identify which features are most predictive of the Product Price.\")\n",
        "print(\"3. Model Complexity vs. Performance:\")\n",
        "print(\"   - Linear Regression is the simplest model but may not capture complex relationships.\")\n",
        "print(\"   - Random Forest and Gradient Boosted Trees are more complex and may perform better on non-linear data.\")\n",
        "print(\"4. Potential Improvements:\")\n",
        "print(\"   - Feature engineering: creating new features or transforming existing ones.\")\n",
        "print(\"   - Hyperparameter tuning: using techniques like cross-validation to optimize model parameters.\")\n",
        "print(\"   - Ensemble methods: combining predictions from multiple models for potentially better performance.\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W87bPB9QShW",
        "outputId": "537bb0ea-9cea-4af0-a364-7f93f693a34c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation Results:\n",
            "-------------------------\n",
            "Metrics: RMSE, MAE, R2\n",
            "Linear Regression: (141.19868977764872, 122.09277418476282, -7.202114260773662e-05)\n",
            "Random Forest: (141.2095145336862, 122.09446598796457, -0.0002253646421315203)\n",
            "Gradient Boosted Trees: (141.2582487086586, 122.11182146462424, -0.0009158785858793816)\n",
            "\n",
            "Analysis and Discussion:\n",
            "------------------------\n",
            "1. Model Performance:\n",
            "   - Comparing RMSE, MAE, and R2 values, we can see which model performs best.\n",
            "   - Lower RMSE and MAE, and higher R2 indicate better performance.\n",
            "2. Feature Importance:\n",
            "   - The Random Forest and Gradient Boosted Trees models can provide feature importance.\n",
            "   - This can help identify which features are most predictive of the Product Price.\n",
            "3. Model Complexity vs. Performance:\n",
            "   - Linear Regression is the simplest model but may not capture complex relationships.\n",
            "   - Random Forest and Gradient Boosted Trees are more complex and may perform better on non-linear data.\n",
            "4. Potential Improvements:\n",
            "   - Feature engineering: creating new features or transforming existing ones.\n",
            "   - Hyperparameter tuning: using techniques like cross-validation to optimize model parameters.\n",
            "   - Ensemble methods: combining predictions from multiple models for potentially better performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand, expr, col, sum, count, min, max, percentile_approx\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"PurchaseTransactions\").getOrCreate()\n",
        "\n",
        "# Step 1: Data Creation\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters, k=length))\n",
        "\n",
        "# Create Customers dataset\n",
        "customers_data = [(i,\n",
        "                   generate_random_string(random.randint(10, 20)),\n",
        "                   random.randint(18, 100),\n",
        "                   random.randint(1, 500),\n",
        "                   random.uniform(100, 10000000))\n",
        "                  for i in range(1, 50001)]\n",
        "\n",
        "customers_df = spark.createDataFrame(customers_data, [\"ID\", \"Name\", \"Age\", \"CountryCode\", \"Salary\"])\n",
        "\n",
        "# Create Purchases dataset\n",
        "purchases_data = [(i,\n",
        "                   random.randint(1, 50000),\n",
        "                   random.uniform(10, 2000),\n",
        "                   random.randint(1, 15),\n",
        "                   generate_random_string(random.randint(20, 50)))\n",
        "                  for i in range(1, 5000001)]\n",
        "\n",
        "purchases_df = spark.createDataFrame(purchases_data, [\"TransID\", \"CustID\", \"TransTotal\", \"TransNumItems\", \"TransDesc\"])\n",
        "\n",
        "# Task 2.0: Load data into storage\n",
        "customers_df.createOrReplaceTempView(\"Customers\")\n",
        "purchases_df.createOrReplaceTempView(\"Purchases\")\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Task 2.1: Filter out purchases above $600\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TEMPORARY VIEW T1 AS\n",
        "    SELECT * FROM Purchases\n",
        "    WHERE TransTotal <= 600\n",
        "\"\"\")\n",
        "\n",
        "print(\"Task 2.1 completed: T1 created\")\n",
        "\n",
        "# Task 2.2: Group by Number of Items and calculate statistics\n",
        "result_2_2 = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        TransNumItems,\n",
        "        percentile_approx(TransTotal, 0.5) AS MedianAmount,\n",
        "        MIN(TransTotal) AS MinAmount,\n",
        "        MAX(TransTotal) AS MaxAmount\n",
        "    FROM T1\n",
        "    GROUP BY TransNumItems\n",
        "    ORDER BY TransNumItems\n",
        "\"\"\")\n",
        "\n",
        "print(\"Task 2.2 Result:\")\n",
        "result_2_2.show()\n",
        "\n",
        "# Task 2.3: Group by customer ID for young customers\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TEMPORARY VIEW T3 AS\n",
        "    SELECT\n",
        "        P.CustID,\n",
        "        C.Age,\n",
        "        SUM(P.TransNumItems) AS TotalItems,\n",
        "        SUM(P.TransTotal) AS TotalAmountSpent\n",
        "    FROM T1 P\n",
        "    JOIN Customers C ON P.CustID = C.ID\n",
        "    WHERE C.Age BETWEEN 18 AND 25\n",
        "    GROUP BY P.CustID, C.Age\n",
        "\"\"\")\n",
        "\n",
        "print(\"Task 2.3 completed: T3 created\")\n",
        "\n",
        "# Task 2.4: Compare customer pairs\n",
        "result_2_4 = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        C1.CustID AS C1ID,\n",
        "        C2.CustID AS C2ID,\n",
        "        C1.Age AS Age1,\n",
        "        C2.Age AS Age2,\n",
        "        C1.TotalAmountSpent AS TotalAmount1,\n",
        "        C2.TotalAmountSpent AS TotalAmount2,\n",
        "        C1.TotalItems AS TotalItemCount1,\n",
        "        C2.TotalItems AS TotalItemCount2\n",
        "    FROM T3 C1\n",
        "    JOIN T3 C2 ON C1.CustID < C2.CustID\n",
        "    WHERE C1.Age < C2.Age\n",
        "        AND C1.TotalAmountSpent > C2.TotalAmountSpent\n",
        "        AND C1.TotalItems < C2.TotalItems\n",
        "\"\"\")\n",
        "\n",
        "print(\"Task 2.4 Result:\")\n",
        "result_2_4.show()\n",
        "\n",
        "# Clean up\n",
        "spark.catalog.dropTempView(\"T1\")\n",
        "spark.catalog.dropTempView(\"T3\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mZdXSPZxfhk",
        "outputId": "b92e1d80-d123-44ed-950b-e51e256835cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully.\n",
            "Task 2.1 completed: T1 created\n",
            "Task 2.2 Result:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand, expr, col, sum, count, min, max, percentile_approx\n",
        "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer, Imputer\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"PurchaseTransactions\").getOrCreate()\n",
        "\n",
        "# Step 1: Data Creation\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters, k=length))\n",
        "\n",
        "# Create Customers dataset\n",
        "customers_data = [(i,\n",
        "                   generate_random_string(random.randint(10, 20)),\n",
        "                   random.randint(18, 100),\n",
        "                   random.randint(1, 500),\n",
        "                   random.uniform(100, 10000000))\n",
        "                  for i in range(1, 50001)]\n",
        "\n",
        "customers_df = spark.createDataFrame(customers_data, [\"ID\", \"Name\", \"Age\", \"CountryCode\", \"Salary\"])\n",
        "\n",
        "# Create Purchases dataset\n",
        "purchases_data = [(i,\n",
        "                   random.randint(1, 50000),\n",
        "                   random.uniform(10, 2000),\n",
        "                   random.randint(1, 15),\n",
        "                   generate_random_string(random.randint(20, 50)))\n",
        "                  for i in range(1, 5000001)]\n",
        "\n",
        "purchases_df = spark.createDataFrame(purchases_data, [\"TransID\", \"CustID\", \"TransTotal\", \"TransNumItems\", \"TransDesc\"])\n",
        "\n",
        "# Task 2.0: Load data into storage\n",
        "customers_df.createOrReplaceTempView(\"Customers\")\n",
        "purchases_df.createOrReplaceTempView(\"Purchases\")\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# Task 2.1: Filter out purchases above $600\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TEMPORARY VIEW T1 AS\n",
        "    SELECT * FROM Purchases\n",
        "    WHERE TransTotal <= 600\n",
        "\"\"\")\n",
        "\n",
        "print(\"Task 2.1 completed: T1 created\")\n",
        "\n",
        "# Task 2.2: Group by Number of Items and calculate statistics\n",
        "result_2_2 = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        TransNumItems,\n",
        "        percentile_approx(TransTotal, 0.5) AS MedianAmount,\n",
        "        MIN(TransTotal) AS MinAmount,\n",
        "        MAX(TransTotal) AS MaxAmount\n",
        "    FROM T1\n",
        "    GROUP BY TransNumItems\n",
        "    ORDER BY TransNumItems\n",
        "\"\"\")\n",
        "\n",
        "print(\"Task 2.2 Result:\")\n",
        "result_2_2.show()\n",
        "\n",
        "# Task 2.3: Group by customer ID for young customers\n",
        "spark.sql(\"\"\"\n",
        "    CREATE OR REPLACE TEMPORARY VIEW T3 AS\n",
        "    SELECT\n",
        "        P.CustID,\n",
        "        C.Age,\n",
        "        SUM(P.TransNumItems) AS TotalItems,\n",
        "        SUM(P.TransTotal) AS TotalAmountSpent\n",
        "    FROM T1 P\n",
        "    JOIN Customers C ON P.CustID = C.ID\n",
        "    WHERE C.Age BETWEEN 18 AND 25\n",
        "    GROUP BY P.CustID, C.Age\n",
        "\"\"\")\n",
        "\n",
        "print(\"Task 2.3 completed: T3 created\")\n",
        "\n",
        "# Task 2.4: Compare customer pairs\n",
        "result_2_4 = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        C1.CustID AS C1ID,\n",
        "        C2.CustID AS C2ID,\n",
        "        C1.Age AS Age1,\n",
        "        C2.Age AS Age2,\n",
        "        C1.TotalAmountSpent AS TotalAmount1,\n",
        "        C2.TotalAmountSpent AS TotalAmount2,\n",
        "        C1.TotalItems AS TotalItemCount1,\n",
        "        C2.TotalItems AS TotalItemCount2\n",
        "    FROM T3 C1\n",
        "    JOIN T3 C2 ON C1.CustID < C2.CustID\n",
        "    WHERE C1.Age < C2.Age\n",
        "        AND C1.TotalAmountSpent > C2.TotalAmountSpent\n",
        "        AND C1.TotalItems < C2.TotalItems\n",
        "\"\"\")\n",
        "\n",
        "print(\"Task 2.4 Result:\")\n",
        "result_2_4.show()"
      ],
      "metadata": {
        "id": "Kgv9f2dnwgVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}